<?xml-stylesheet href="/rss.xsl" type="text/xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lokadhatu Myriadis</title>
    <link>https://lokadhatu.com/en/</link>
    <description>Recent content on Lokadhatu Myriadis</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>2015-2024 Lokadhatu Myriadis</copyright>
    <lastBuildDate>Sat, 21 Sep 2024 18:50:00 +0100</lastBuildDate>
    
        <atom:link href="https://lokadhatu.com/en/index.xml" rel="self" type="application/rss+xml" />
    
    
    
    
    
        <item>
        <title>Chain-of-Thought</title>
        <link>https://lokadhatu.com/en/posts/2024-09-21-chain-of-thought/</link>
        <pubDate>Sat, 21 Sep 2024 18:50:00 +0100</pubDate>
        
        <guid>https://lokadhatu.com/en/posts/2024-09-21-chain-of-thought/</guid>
        <description>Lokadhatu Myriadis https://lokadhatu.com/en/posts/2024-09-21-chain-of-thought/ -&lt;p&gt;Reasoning has long been regarded as a cornerstone of intelligence, traditionally dominated by symbolic approaches (Russell &amp;amp; Norvig, 2016). The recent launch of &lt;a href=&#34;https://openai.com/index/introducing-openai-o1-preview/&#34;&gt;o1 series&lt;/a&gt; by OpenAI marked  the productionalization of large language model (LLM)-based reasoning systems.  At the core of these systems lies a key yet simple technique known as &lt;em&gt;chain of thought&lt;/em&gt; (CoT). This blog explores the concept of chain-of-thought reasoning, tracing its origins, examining its variants, and uncovering its limitations.  Our discussion summarizes the insights shared by Denny Zhou in his Berkeley &lt;a href=&#34;https://llmagents-learning.org/f24&#34;&gt;lecture&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;ideas&#34;&gt;Ideas&lt;/h2&gt;
&lt;p&gt;The term &lt;em&gt;CoT&lt;/em&gt; is introduced by &lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;&gt;Jason Wei et al. (2022)&lt;/a&gt;. It simply means a series of intermediate reasoning steps that lead to the final answer for a problem. Similar concepts have been explored under different terms,  such as &lt;em&gt;rationale&lt;/em&gt; (&lt;a href=&#34;http://aclweb.org/anthology/P17-1015&#34;&gt;Ling et al. 2017&lt;/a&gt;), &lt;em&gt;natural language solutions&lt;/em&gt; (&lt;a href=&#34;http://arxiv.org/abs/2110.14168&#34;&gt;Cobbe et al. 2021&lt;/a&gt;), or
&lt;em&gt;scratchpad&lt;/em&gt; (&lt;a href=&#34;http://arxiv.org/abs/2112.00114&#34;&gt;Nye et al. 2021&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;A simple example of chain-of-thought reasoning is as follows:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Problem:
John has 5 apples. He buys 3 more apples from the store and then gives 2 apples to his friend. How many apples does John have left?

Without Chain-of-Thought:
Output: &amp;#34;John has 6 apples.&amp;#34;
(Note: Correct, but reasoning is not provided.)

With Chain-of-Thought:
1. John starts with 5 apples.
2. He buys 3 more apples. Adding them together: 5 + 3 = 8.
3. He gives 2 apples to his friend. Subtracting that: 8 - 2 = 6.
4. Final Answer: &amp;#34;John has 6 apples left.&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The use of intermediate steps to solve problems was pioneered by &lt;a href=&#34;http://aclweb.org/anthology/P17-1015&#34;&gt;Ling et al. (2017)&lt;/a&gt; at DeepMind. Their work involved training a sequence-to-sequence model from scratch using a novel algebraic word problem dataset that included explicitly collected intermediate steps. &lt;a href=&#34;http://arxiv.org/abs/2110.14168&#34;&gt;Cobbe et al. (2021)&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;#myfootnote1&#34;&gt;1&lt;/a&gt;&lt;!-- raw HTML omitted --&gt; at OpenAI followed the idea by creating a much larger math word problem dataset with intermediate steps, which they used to finetune the &lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;GPT-3&lt;/a&gt; model &lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;#myfootnote2&#34;&gt;2&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;.&lt;/p&gt;
&lt;p&gt;By combining the strengths  of the use of intermediate steps and the in-context few shot learning via &lt;em&gt;prompting&lt;/em&gt; technique introduced by Brown et al (2020) in GPT-3, Wei et al. (2022) discovered that few-shot CoT prompting significantly improves the reasoning capabilities of larger language models &lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;#myfootnote3&#34;&gt;3&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;. By providing a few examples of problems along with their step-by-step reasoning in the input prompt, they demonstrated that larger models could emulate this reasoning structure and apply it to new problems. This method proved particularly effective in tasks requiring arithmetic, logical inference, and symbolic reasoning, achieving notable performance improvements over traditional prompting approaches.&lt;/p&gt;
&lt;p&gt;More surprisingly, &lt;a href=&#34;http://arxiv.org/abs/2205.11916&#34;&gt;Kojima et al. (2022)&lt;/a&gt; in their work &lt;em&gt;zero-shot CoT prompting&lt;/em&gt; demonstrates that explicit examples are not always necessary to elicit reasoning. They found that appending a simple instruction like &amp;ldquo;Let&amp;rsquo;s think step by step&amp;rdquo; to the problem prompt could trigger models to generate intermediate reasoning steps, even without additional labeled examples.&lt;/p&gt;
&lt;p&gt;More recently, &lt;a href=&#34;http://arxiv.org/abs/2402.10200&#34;&gt;Wang and Zhou (2024)&lt;/a&gt; discovered that even prompt is not necessary. They proposed CoT-decoding which leverages the LLM&amp;rsquo;s confidence scores during the decoding process to select the most reliable CoT path to bypass prompting altogether.&lt;/p&gt;
&lt;h2 id=&#34;improvements&#34;&gt;Improvements&lt;/h2&gt;
&lt;p&gt;Several techniques for improve the CoT prompting have been proposed, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Self-Consistency (&lt;a href=&#34;http://arxiv.org/abs/2203.11171&#34;&gt;Wang, et al. 2023&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;This approach improves CoT reasoning by generating multiple reasoning path and selecting the most consistent answer based on a voting mechanism or other criteria&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Least-to-Most Prompting (&lt;a href=&#34;http://arxiv.org/abs/2205.10625&#34;&gt;Zhou et al. 2023&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;This technique breaks down complex problems into simpler, manageable subproblems, allowing LLMs to solve more challenging tasks than those presented in the prompt itself&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Analogical Prompting (&lt;a href=&#34;http://arxiv.org/abs/2310.01714&#34;&gt;Yasunaga et al. 2024&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;This approach guides LLMs to generate their own relevant examples and knowledge, leading to improved generalization and reasoning capabilities&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;Despite its strengths, CoT reasoning has several limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Irrelevant  Context (&lt;a href=&#34;https://arxiv.org/abs/2302.00093&#34;&gt;Shi et al. 2023&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;LLMs can be easily distracted by irrelevant or extraneous information within the input, leading to reasoning errors or incorrect conclusions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Limited Self-Correction (&lt;a href=&#34;https://arxiv.org/abs/2310.01798&#34;&gt;Huang et al. 2023&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;LLMs struggle to self-correct their reasoning without external feedback or oracle labels, highlighting a need for improved intrinsic self-correction capabilities&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Premise Order Sensitivity (&lt;a href=&#34;https://arxiv.org/abs/2402.08939&#34;&gt;Chen et al. 2024&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;LLMs exhibit sensitivity to the order in which information is presented, impacting reasoning performance even when the underlying information remains the same&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;1&lt;!-- raw HTML omitted --&gt;: Many authors of the GSM8K paper are also the key contributors to o1.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;: Even though one of the main focuses of the paper is to demonstrate that a verifier, trained on a smaller GPT-3 model to judge the correctness of model-generated solutions, can achieve comparable or better performance than fine-tuning a 30x larger GPT-3 model, particularly by leveraging verification&amp;rsquo;s favorable scaling with data size.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;3&lt;!-- raw HTML omitted --&gt;: They note that the benefits of CoT prompting are limited for smaller models.&lt;/p&gt;
- https://lokadhatu.com/en/posts/2024-09-21-chain-of-thought/ - 2015-2024 Lokadhatu Myriadis</description>
        </item>
    
    
  </channel>
</rss> 